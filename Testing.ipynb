{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNprFcq27MBj"
   },
   "source": [
    "## Introduction\n",
    "There are 2 types of tests:\n",
    "1. An integration test checks that components in your application operate with each other.\n",
    "2. A unit test checks a small component in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT-teGj93Esp"
   },
   "source": [
    "This will not output anything on the REPL because the values are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "482qreAy2v38"
   },
   "outputs": [],
   "source": [
    "assert sum([1,2,3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18msoLGn3Hvj"
   },
   "source": [
    "If the result from sum() is incorrect, this will fail with an AssertionError and the message \"Should be 6\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "-RaTG1BL25JD",
    "outputId": "f5013105-445c-44ce-e654-a14e0a7d5c49"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa10351dd942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "assert sum([1, 1, 1]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJdXS67R3Sz8"
   },
   "source": [
    "Instead of testing on the REPL, you’ll want to put this into a new Python file called test_sum.py and execute it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3xnRROOQ3spl",
    "outputId": "2b73699a-6fcb-4efe-ebb8-367688869ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything passed\n"
     ]
    }
   ],
   "source": [
    "! python test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uStjFv2I4wH9"
   },
   "source": [
    "You tested with a list. Now test with a tuple as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "z4jIIFue3s4v",
    "outputId": "12900929-5970-47d8-8af8-deebc22c8eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"test_sum_2.py\", line 9, in <module>\n",
      "    test_sum_tuple()\n",
      "  File \"test_sum_2.py\", line 5, in test_sum_tuple\n",
      "    assert sum((1, 2, 2)) == 6, \"Should be 6\"\n",
      "AssertionError: Should be 6\n"
     ]
    }
   ],
   "source": [
    "! python test_sum_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-yetUKl5Fcl"
   },
   "source": [
    "Writing tests in this way is okay for a simple check, but what if more than one fails? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGsypDvq6cwt"
   },
   "source": [
    "## Test Runner\n",
    "The test runner is a special application designed for running tests, checking the output, and giving you tools for debugging and diagnosing tests and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM7j43L-8Hl0"
   },
   "source": [
    "### Unittest\n",
    "unittest contains both a testing framework and a test runner. unittest requires that:\n",
    "- You put your tests into classes as methods\n",
    "- You use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement\n",
    "\n",
    "Let's create a test_sum_unittest.py file by following the steps below:\n",
    "1. Import unittest from the standard library\n",
    "2. Create a class called TestSum that inherits from the TestCase class\n",
    "3. Convert the test functions into methods by adding self as the first argument\n",
    "4. Change the assertions to use the self.assertEqual() method on the TestCase class\n",
    "5. Change the command-line entry point to call unittest.main()\n",
    "\n",
    "one success (indicated with .) and one failure (indicated with F):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "gjP1eLM36X2b",
    "outputId": "b581e417-26c5-49f2-c85a-ea774020c538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F\n",
      "======================================================================\n",
      "FAIL: test_sum_tuple (__main__.TestSum)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"test_sum_unittest.py\", line 10, in test_sum_tuple\n",
      "    self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
      "AssertionError: 5 != 6 : Should be 6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.000s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "! python test_sum_unittest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN00favYA8_p"
   },
   "source": [
    "### Nose2\n",
    "You may find that over time, as you write hundreds or even thousands of tests for your application, it becomes increasingly hard to understand and use the output from unittest. nose2 is compatible with any tests written using the unittest framework and can be used as a drop-in replacement for the unittest test runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "sSwBQqDg5F7p",
    "outputId": "bfe02873-1fff-4297-f20f-115a207a2a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".FF....F\n",
      "======================================================================\n",
      "FAIL: test_sum_2.transplant_class.<locals>.C (test_sum_tuple)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bilgi/projects/Testing-in-Python/test_sum_2.py\", line 5, in test_sum_tuple\n",
      "    assert sum((1, 2, 2)) == 6, \"Should be 6\"\n",
      "AssertionError: Should be 6\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_sample.transplant_class.<locals>.C (test_answer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bilgi/projects/Testing-in-Python/test_sample.py\", line 7, in test_answer\n",
      "    assert inc(3) == 5\n",
      "AssertionError\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_sum_tuple (test_sum_unittest.TestSum)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bilgi/projects/Testing-in-Python/test_sum_unittest.py\", line 10, in test_sum_tuple\n",
      "    self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
      "AssertionError: 5 != 6 : Should be 6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.001s\n",
      "\n",
      "FAILED (failures=3)\n"
     ]
    }
   ],
   "source": [
    "! python -m nose2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pg-qRHzmBuzS"
   },
   "source": [
    "### pytest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ2Hd81WCe0j"
   },
   "source": [
    "pytest supports execution of unittest test cases. The real advantage of pytest comes by writing pytest test cases. pytest test cases are a series of functions in a Python file starting with the name test_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "5hfUNzn-45Ga",
    "outputId": "4c7775b0-e05c-4a56-aacd-398bcfd147c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-5.4.3, py-1.8.2, pluggy-0.13.1\n",
      "rootdir: /home/bilgi/projects/Testing-in-Python\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_sample.py \u001b[31mF\u001b[0m\u001b[31m                                                         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_answer __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_answer\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m inc(\u001b[94m3\u001b[39;49;00m) == \u001b[94m5\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 4 = inc(3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:7: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_sample.py::test_answer - assert 4 == 5\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest test_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your application is a single script, you can import any attributes of the script, such as classes, functions, and variables, using the built-in __import __ () function.\n",
    "```python\n",
    "target = __import__(\"my_sum.py\")\n",
    "sum = target.sum\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Structure a Simple Test\n",
    "\n",
    "You’ll want to first make a couple of decisions:\n",
    "1. What do you want to test?\n",
    "2. Are you writing a unit test or an integration test?\n",
    "\n",
    "Workflow:\n",
    "1. Create your inputs\n",
    "2. Execute the code being tested, capturing the output\n",
    "3. Compare the output with an expected result\n",
    "\n",
    "For this application, you’re testing mysum:\n",
    "1. Can it sum a list of whole numbers (integers)?\n",
    "2. Can it sum a tuple or set?\n",
    "3. Can it sum a list of floats?\n",
    "4. What happens when you provide it with a bad value, such as a single integer or a string?\n",
    "5. What happens when one of the values is negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```unittest.main()``` ->  This executes the test runner by discovering all classes in this file that inherit from unittest.TestCase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run test_my_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is using the unittest command line. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bad_type (test_my_sum.TestSum) ... ok\n",
      "test_list_int (test_my_sum.TestSum) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "! python -m unittest -v test_my_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All have opposite methods, named .assertIsNot(), and so forth. Some of the most commonly used methods in unittest:\n",
    "\n",
    "| Method                  | Equivalent to    |\n",
    "|-------------------------|------------------|\n",
    "| .assertEqual(a, b)      | a == b           |\n",
    "| .assertTrue(x)          | bool(x) is True  |\n",
    "| .assertFalse(x)         | bool(x) is False |\n",
    "| .assertIs(a, b)         | a is b           |\n",
    "| .assertIsNone(x)        | x is None        |\n",
    "| .assertIn(a, b)         | a in b           |\n",
    "| .assertIsInstance(a, b) | isinstance(a, b) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will search the current directory for any files named test*.py and attempt to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...F\n",
      "======================================================================\n",
      "FAIL: test_sum_tuple (test_sum_unittest.TestSum)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bilgi/projects/Testing-in-Python/test_sum_unittest.py\", line 10, in test_sum_tuple\n",
      "    self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
      "AssertionError: 5 != 6 : Should be 6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.000s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "! python -m unittest discover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your source code is not in the directory root and contained in a subdirectory, for example in a folder called src/\n",
    "\n",
    "```\n",
    "$ python -m unittest discover -s tests -t src\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that you create as an input is known as a **fixture**. If you’re running the same test and passing different values each time and expecting the same result, this is known as **parameterization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Expected Failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a special way to handle expected errors. You can use `.assertRaises()` as a context-manager, then inside the with block execute the test steps:\n",
    "```python\n",
    "def test_bad_type(self):\n",
    "    data = \"banana\"\n",
    "    with self.assertRaises(TypeError):\n",
    "        result = sum(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bad_type (test_my_sum.TestSum) ... ok\n",
      "test_list_int (test_my_sum.TestSum) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "! python -m unittest -v test_my_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Integration Tests\n",
    "**Integration testing** is the testing of multiple components of the application to check that they work together. Integration testing might require acting like a consumer or user of the application by:\n",
    "- Calling an HTTP REST API\n",
    "- Calling a Python API\n",
    "- Calling a web service\n",
    "- Running a command line\n",
    "\n",
    "Integration tests are check more components and therefore that have **more side effects** than a unit test. Also, integration tests will require **more fixtures** to be in place, like:\n",
    "- a database,\n",
    "- a network socket, \n",
    "- a configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6hzYtI73tGK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hx2Sc1eM3tJA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgTw6ph23tK3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNg0rgRoGmennDN0JfHAzY8",
   "collapsed_sections": [
    "dNprFcq27MBj",
    "BM7j43L-8Hl0",
    "vN00favYA8_p"
   ],
   "include_colab_link": true,
   "name": "Testing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
